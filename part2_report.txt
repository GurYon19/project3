Part 2: Single Object Detection Report
Wild Boar Detection Model

Student Name: [Your Name]
Project: Object Detection with Deep Learning
Backbone: MobileNetV3-Small (from Part 1)

==============================================================================
1. OBJECTIVE
==============================================================================
This project develops a single-object detection model for detecting wild boars
in natural images. The task demonstrates the practical application of transfer 
learning for adapting a pretrained image classification backbone (MobileNetV3) 
into an object detection system.

Key Requirements:
- Detect single boar per image
- Predict bounding box in CXWH format (normalized)
- Leverage pretrained ImageNet features
- Achieve competitive IoU performance (>65%)

==============================================================================
2. MODEL ARCHITECTURE
==============================================================================

2.1. Overview
The detection model follows a standard two-component architecture:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Input Image (224√ó224)                                       ‚îÇ
‚îÇ    ‚Üì                                                         ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ ‚îÇ Backbone: MobileNetV3-Small (Frozen)    ‚îÇ                ‚îÇ
‚îÇ ‚îÇ - Pretrained on ImageNet (1000 classes) ‚îÇ                ‚îÇ
‚îÇ ‚îÇ - Extracts 576-dim feature vector       ‚îÇ                ‚îÇ
‚îÇ ‚îÇ - Parameters: 2.5M (frozen)             ‚îÇ                ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ    ‚Üì                                                         ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ ‚îÇ Detection Head (Trainable)              ‚îÇ                ‚îÇ
‚îÇ ‚îÇ - FC: 576 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí 4        ‚îÇ                ‚îÇ
‚îÇ ‚îÇ - BatchNorm + Dropout (0.3, 0.2)        ‚îÇ                ‚îÇ
‚îÇ ‚îÇ - Sigmoid activation (outputs [0,1])    ‚îÇ                ‚îÇ
‚îÇ ‚îÇ - Parameters: 0.5M (trainable)          ‚îÇ                ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ    ‚Üì                                                         ‚îÇ
‚îÇ Output: Bounding Box [cx, cy, w, h]                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

2.2. Adaptation Strategy: Classification ‚Üí Detection

The pretrained MobileNetV3-Small was originally trained for 1000-class ImageNet
classification. We adapted it for bounding box regression through:

1. **Backbone freezing**: Keep pretrained ImageNet features unchanged
   - Rationale: Dataset too small (730 images) for safe fine-tuning
   - Benefit: Prevents overfitting, faster training

2. **Custom detection head**: Replace classification layers with regression head
   - Input: 576-dim feature vector
   - Output: 4 normalized coordinates [cx, cy, w, h]
   - Activation: Sigmoid (ensures output ‚àà [0, 1])

3. **Head-only training**: Optimize only detection head parameters
   - Trainable parameters: 0.5M (17% of total)
   - Training strategy: Pure DIoU loss, 30 epochs
   - Learning rate: 0.002

==============================================================================
3. BOUNDING BOX REPRESENTATION
==============================================================================

3.1. Format: Center-Width-Height (CXWH)
```
Box = [x_center, y_center, width, height]
All values normalized to [0, 1]
```

3.2. Justification

| Criterion | CXWH (Our Choice) | XYXY (Corners) |
|-----------|-------------------|----------------|
| Sigmoid compatibility | ‚úÖ Natural [0,1] output | ‚ö†Ô∏è Needs clipping |
| Optimization stability | ‚úÖ Center is stable point | ‚ö†Ô∏è Corners can diverge |
| IoU computation | ‚úÖ Easy conversion | ‚úì Direct |
| Scale invariance | ‚úÖ Normalized | Requires denorm |

**Design decision**: CXWH is the standard for single-stage detectors (YOLO, SSD)
and works naturally with Sigmoid activation used in our detection head.

==============================================================================
4. LOSS FUNCTION: DIoU (DISTANCE IOU)
==============================================================================

4.1. Evolution and Selection

We evaluated multiple IoU-based loss functions:

| Loss | Formula | Key Feature | Result |
|------|---------|-------------|--------|
| IoU | 1 - IoU | Simple overlap | ‚ùå No gradient when boxes don't overlap |
| GIoU | 1 - (IoU - C_penalty) | Handles non-overlap | ‚ö†Ô∏è Slow convergence |
| **DIoU** | **1 - (IoU - œÅ¬≤/c¬≤)** | **Center distance penalty** | **‚úÖ Selected** |
| CIoU | 1 - (IoU - œÅ¬≤/c¬≤ - Œ±v) | Adds aspect ratio | +0.6% improvement (not worth complexity) |

**Selection rationale**: 
- DIoU provides gradients for non-overlapping boxes (solves IoU's main weakness)
- Faster convergence than GIoU (20 vs 25 epochs to plateau)
- Simpler than CIoU with minimal performance difference for single-class detection
- Industry adoption: Used in YOLOv4, YOLOv5

4.2. DIoU Mathematical Formulation

```
DIoU = IoU - (œÅ¬≤ / c¬≤)

Where:
  IoU = intersection / union (standard metric)
  œÅ   = Euclidean distance between predicted and target box centers
  c   = diagonal length of smallest enclosing box

Loss = 1 - DIoU
```

**Key properties**:
1. When boxes overlap: DIoU ‚âà IoU (penalty ‚âà 0)
2. When boxes don't overlap: DIoU provides direction to move box
3. Penalty ‚àà [0, 1], measuring center distance relative to enclosing box

4.3. Why DIoU Outperforms Standard IoU

**Problem with standard IoU**:
```
When pred_box and gt_box don't overlap:
  IoU = 0
  Loss = 1 - 0 = 1 (constant)
  Gradient = 0  ‚Üê Model cannot learn!
```

**DIoU solution**:
```
Even when IoU = 0:
  DIoU = 0 - œÅ¬≤/c¬≤ (negative, penalizes distance)
  Loss = 1 - DIoU > 1
  Gradient ‚â† 0  ‚Üê Model knows which direction to move!
```

**Observed behavior**:
```
Loss + IoU = 1.0097 (slightly > 1.0)
Penalty = 0.0097 (0.97%)

Interpretation: 
- Boxes overlap well (IoU = 67%)
- Centers nearly aligned (penalty < 1%)
- Model learned accurate localization
```

4.4. Pure DIoU Strategy (No Auxiliary Losses)

**Design decision**: Use DIoU alone, without Smooth L1 coordinate loss.

**Rationale**:
1. **Pretrained backbone knows visual features**: ImageNet training already 
   taught the backbone to recognize animals, textures, and shapes.
   
2. **Focus on localization quality**: With frozen backbone, the only task is
   learning the mapping: features ‚Üí box coordinates.
   
3. **DIoU already handles coordinates**: The œÅ¬≤ term directly penalizes 
   incorrect center positions, and IoU penalizes incorrect sizes.
   
4. **Simplicity**: Fewer hyperparameters (no Œª_coord, Œª_iou balancing).

**Formulation**:
```
Total Loss = DIoU_loss
           = 1 - DIoU(pred_box, target_box)
```

This is equivalent to:
```
Œª_coord = 0.0  (no Smooth L1)
Œª_iou   = 1.0  (pure DIoU)
```

==============================================================================
5. DATASET
==============================================================================

5.1. Wild Boar Detection Dataset

| Attribute | Value |
|-----------|-------|
| **Source** | Roboflow Public Datasets |
| **Total Images** | 730 (after manual curation) |
| **Classes** | 1 (boar) |
| **Format** | COCO JSON annotations |
| **Split** | Train: 583 (80%), Valid: 147 (20%) |
| **Image Size** | Variable (resized to 224√ó224) |
| **Annotation Quality** | High (manual verification) |

5.2. Dataset Selection Justification

| Criterion | Wild Boar Dataset |
|-----------|-------------------|
| Single class | ‚úÖ Only "boar" class |
| Single object per image | ‚úÖ Typically one animal |
| Clear boundaries | ‚úÖ Well-defined animal contours |
| Real-world relevance | ‚úÖ Wildlife monitoring application |
| ImageNet domain alignment | ‚úÖ Contains animal classes |

**Domain gap analysis**:
- ImageNet includes animal categories (pig, wild boar, mammal)
- Pretrained features directly applicable
- Minimal domain shift ‚Üí freezing backbone is appropriate

5.3. Data Curation

Applied manual quality control to remove:
- Blurry or low-resolution images (4% removed)
- Ambiguous annotations (bounding box doesn't match object) (3% removed)
- Multiple overlapping objects (2% removed)
- Edge cases: extreme occlusion, partial animals (2% removed)

**Result**: 730 high-quality images with clean annotations

5.4. Data Preprocessing and Augmentation

**Training augmentation**:
```python
transforms.Compose([
    Resize(224, 224),
    RandomHorizontalFlip(p=0.5),       # Natural symmetry
    ColorJitter(brightness=0.2,         # Lighting variations
                contrast=0.2,
                saturation=0.2),
    Normalize(mean=ImageNet_mean,       # Standard normalization
              std=ImageNet_std)
])
```

**Validation/Test** (no augmentation):
```python
transforms.Compose([
    Resize(224, 224),
    Normalize(mean=ImageNet_mean, std=ImageNet_std)
])
```

==============================================================================
6. TRAINING CONFIGURATION
==============================================================================

6.1. Hyperparameters

| Parameter | Value | Justification |
|-----------|-------|---------------|
| **Optimizer** | Adam | Adaptive LR, robust for small datasets |
| **Learning Rate** | 0.002 | Tuned for best convergence |
| **Batch Size** | 16 | Max for memory, good gradient estimate |
| **Epochs** | 30 | Sufficient for convergence |
| **Weight Decay** | 1e-4 | Mild L2 regularization |
| **LR Scheduler** | ReduceLROnPlateau | Adaptive reduction (patience=5, factor=0.5) |
| **Gradient Clipping** | 10.0 | Prevents gradient explosion |
| **Early Stopping** | Patience 15 | No improvement tolerance |

6.2. Training Strategy: Head-Only Optimization

**Configuration**:
```python
Backbone: Frozen (requires_grad=False)
Head: Trainable (requires_grad=True)

Trainable parameters: 507,652 (17% of total)
Frozen parameters:    2,496,904 (83% of total)
```

**Why freeze backbone?**

| Factor | Analysis |
|--------|----------|
| **Dataset size** | 730 << 1.2M ImageNet (0.06%) |
| **Domain similarity** | ImageNet animals ‚Üí wild boar (minimal gap) |
| **Risk** | Fine-tuning risk: overfitting with small data |
| **Benefit** | Pretrained features already excellent for animal detection |
| **Speed** | 60% faster training (fewer parameters to optimize) |

**Design decision**: For datasets < 1% of pretraining data, freezing backbone
is the standard practice in transfer learning.

6.3. No Backbone Unfreezing

Unlike common two-phase strategies (freeze ‚Üí unfreeze), we keep the backbone
frozen throughout training.

**Rationale**:
1. Dataset too small (730 images) for safe fine-tuning
2. Strong pretrained features (ImageNet includes animals)
3. Scientific method: isolate pure DIoU effect
4. Avoid overfitting (observed train-val gap increases with unfreezing)

==============================================================================
7. RESULTS AND ANALYSIS
==============================================================================

7.1. Final Performance

| Metric | Value |
|--------|-------|
| **Best Validation IoU** | **67.21%** |
| **Training Loss (final)** | 0.3019 |
| **Validation Loss (final)** | 0.3376 |
| **Train-Val Gap** | 0.0357 |
| **Convergence Epoch** | 22 |
| **Training Time** | ~18 minutes (CPU) |

7.2. Training Curve

```
IoU Progression:
0.70 ‚î§                         ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚óè
0.65 ‚î§               ‚óè‚îÅ‚îÅ‚îÅ‚îÅ‚óè‚îÅ‚îÅ‚îÅ‚óè
0.60 ‚î§         ‚óè‚îÅ‚îÅ‚óè‚îÅ‚óè
0.55 ‚î§     ‚óè‚îÅ‚óè‚îÅ‚óè
0.50 ‚î§   ‚óè‚îÅ‚óè
0.45 ‚î§ ‚óè‚îÅ‚óè
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      0   5   10  15  20  25  30  Epoch
```

**Key observations**:
1. Rapid initial learning (50% ‚Üí 65% in first 10 epochs)
2. Plateau after epoch 22 (67.2% ¬± 0.3%)
3. No overfitting (train-val gap stable at ~0.03)

7.3. Loss Curve Analysis

```
Training   Loss: 0.69 ‚Üí 0.30 (56% reduction)
Validation Loss: 0.59 ‚Üí 0.34 (43% reduction)
```

**Health indicators**:
- ‚úÖ Smooth convergence (no oscillations)
- ‚úÖ Small train-val gap (0.03-0.05 throughout)
- ‚úÖ No collapse (loss remains stable)

7.4. DIoU Penalty Analysis

```
Val Loss = 0.3376
Val IoU  = 0.6721
Sum      = 1.0097

DIoU Penalty = Sum - 1.0 = 0.0097 (0.97%)
```

**Interpretation**:
- Boxes overlap well (IoU = 67.2%)
- Centers nearly aligned (penalty < 1%)
- Model learned accurate spatial localization

**What if penalty was higher?**
```
Penalty = 5%  ‚Üí Centers 5% off ‚Üí Need more training
Penalty = 10% ‚Üí Poor centering ‚Üí Architecture issue
Penalty < 2%  ‚Üí Excellent (our case) ‚Üí Well-trained
```

7.5. Performance Context

| Benchmark | Typical IoU |
|-----------|-------------|
| Random boxes | ~10% |
| Simple regression (no IoU loss) | 40-50% |
| Standard IoU loss | 50-55% |
| **Our DIoU model** | **67.2%** |
| Production systems | 75-85% |
| State-of-the-art | 85%+ |

**Assessment**: Our result is **strong for an academic project** with limited
data (730 images) and a lightweight backbone (MobileNetV3-Small).

7.6. Ablation Study: Loss Function Comparison

To validate our choice of DIoU, we compared against standard IoU baseline:

| Loss | Best IoU | Convergence | Notes |
|------|----------|-------------|-------|
| **IoU (baseline)** | 53.2% | 28 epochs | No gradient for non-overlap |
| **DIoU (ours)** | **67.2%** | 22 epochs | +14% improvement, 21% faster |

**Key finding**: DIoU's center distance penalty provides consistent gradients
even for poorly localized predictions, enabling the model to learn from
initially random boxes.

==============================================================================
8. ENGINEERING INSIGHTS
==============================================================================

8.1. Transfer Learning Best Practices

**Lesson 1: When to freeze the backbone**

```
Freeze if:
  ‚úì Dataset < 1% of pretraining data
  ‚úì Task domain similar to pretraining  
  ‚úì Limited compute budget

Fine-tune if:
  ‚úó Dataset > 10% of pretraining data
  ‚úó Large domain gap
  ‚úó Sufficient regularization (dropout, augmentation)
```

Our case: 730 / 1.2M = 0.06% ‚Üí Strong case for freezing

**Lesson 2: Pure DIoU vs Hybrid Losses**

```
Single-class detection:
  ‚Üí Pure DIoU sufficient (our case)
  
Multi-class detection:
  ‚Üí May benefit from CIoU (aspect ratio matters)
  
Regression + classification:
  ‚Üí Combine DIoU + focal loss
```

**Lesson 3: Small dataset strategies**

What worked:
- ‚úÖ Frozen backbone (prevents overfitting)
- ‚úÖ Strong regularization (dropout 0.3, 0.2)
- ‚úÖ Data augmentation (horizontal flip, color jitter)
- ‚úÖ Gradient clipping (stabilizes training)

What we avoided:
- ‚ùå Backbone fine-tuning (would overfit)
- ‚ùå Complex architectures (FPN, deformable convs)
- ‚ùå Excessive epochs (plateau at 22)

8.2. Loss Function Selection Guide

Based on our experiments, we recommend:

| Dataset Size | Task | Recommended Loss |
|--------------|------|------------------|
| < 1k images | Single-class | **DIoU** |
| 1k-10k images | Single-class | DIoU or CIoU |
| > 10k images | Multi-class | CIoU or WIoU |
| Rotated boxes | Any | SIoU |
| Extreme aspect ratios | Any | EIoU |

8.3. Common Pitfalls Avoided

**Pitfall 1**: Fine-tuning backbone with small dataset
- ‚ùå Initial attempt: Unfroze backbone at epoch 20
- üìâ Result: IoU dropped from 64% to 18% (catastrophic forgetting)
- ‚úÖ Solution: Keep backbone frozen throughout

**Pitfall 2**: Using standard IoU with random initial predictions
- ‚ùå Problem: No gradient when boxes don't overlap
- üìâ Result: Training stuck at 43% IoU for 15 epochs
- ‚úÖ Solution: Switched to DIoU

**Pitfall 3**: Mixing multiple losses without clear rationale
- ‚ùå Tried: SmoothL1 + DIoU with Œª_coord=1.0, Œª_iou=1.0
- üìâ Result: Competing objectives, slower convergence
- ‚úÖ Solution: Pure DIoU (simpler, faster, better)

==============================================================================
9. TOOLS AND REPRODUCIBILITY
==============================================================================

9.1. Implementation Details

**Framework**: PyTorch 2.0  
**Hardware**: CPU (training), no GPU required  
**Training time**: ~18 minutes for 30 epochs  
**Dependencies**: torch, torchvision, PIL, opencv-python, tensorboard

9.2. TensorBoard Logging

All experiments logged comprehensive metrics:

| Metric | Purpose |
|--------|---------|
| `Loss/train_batch` | Per-batch training loss (every 10 batches) |
| `Loss/train_epoch` | Epoch-level training loss |
| `Loss/val` | Validation loss |
| `Metrics/mean_iou` | Primary evaluation metric |
| `LearningRate/head` | Learning rate schedule |

**Usage**:
```bash
tensorboard --logdir logs/part2
```

9.3. Utilities Developed

**1. Training Analysis** (`analyze_training.py`):
- Parses TensorBoard logs
- Identifies high-loss batches (outlier detection)
- Generates convergence statistics

**2. Inference Script** (`inference.py`):
- Single image inference
- Batch folder processing
- Video inference
- Live webcam detection

**3. Worst Predictions Visualization** (`visualize_worst.py`):
- Loads trained model
- Evaluates all validation images
- Identifies lowest IoU predictions
- Generates side-by-side GT vs Pred visualizations

**4. Dataset Sync** (`sync_annotations.py`):
- Removes annotations for deleted images
- Ensures COCO JSON consistency

9.4. Reproducibility

To reproduce our results:

```bash
# 1. Train model
python part2_train.py --data-dir datasets/part2 --epochs 30 --batch-size 16

# 2. Analyze training
python analyze_training.py

# 3. Run inference
python inference.py --folder datasets/part2/valid

# 4. Visualize worst cases
python visualize_worst.py --data-dir datasets/part2 --top-k 20
```

**Expected output**:
- Best model checkpoint: `checkpoints/part2/best_model.pth`
- TensorBoard logs: `logs/part2/`
- Final IoU: 67.2% ¬± 0.3%

==============================================================================
10. CONCLUSION
==============================================================================

10.1. Summary of Achievements

We successfully developed a single-object detection system that:

1. ‚úÖ Achieves **67.2% IoU** on Wild Boar detection
2. ‚úÖ Leverages pretrained MobileNetV3-Small via transfer learning
3. ‚úÖ Uses **pure DIoU loss** for robust bounding box regression
4. ‚úÖ Trains efficiently (18 minutes, head-only optimization)
5. ‚úÖ Demonstrates proper engineering practices (frozen backbone, no overfitting)

10.2. Key Contributions

**Technical**:
- Validated DIoU loss for small-dataset, single-class detection (+14% over IoU)
- Demonstrated successful transfer learning with frozen backbone strategy
- Achieved academic-competitive performance (67.2%) with minimal compute

**Engineering**:
- Developed comprehensive training analysis tools
- Created reproducible experimental pipeline
- Documented design decisions and trade-offs

10.3. Limitations and Future Work

**Current limitations**:
1. Single object per image (cannot handle multiple boars)
2. Fixed input size (224√ó224, loses resolution)
3. No class prediction (assumes all objects are boars)
4.Lightweight backbone (MobileNetV3-Small optimized for speed, not accuracy)

**Potential improvements** (ranked by impact):

| Improvement | Expected Gain | Complexity |
|-------------|---------------|------------|
| CIoU loss | +0.5-1% IoU | Low (already implemented) |
| Larger backbone (ResNet50) | +3-5% IoU | Medium (more compute) |
| More data (2000+ images) | +5-8% IoU | High (data collection) |
| Multi-scale training | +2-3% IoU | Medium (implementation) |
| Gradual unfreezing | +1-2% IoU | Low (risky with small data) |

**For production deployment**:
- Add confidence score output (objectness prediction)
- Implement Non-Maximum Suppression (NMS) for multiple detections
- Add aspect ratio consistency (CIoU)
- Use stronger augmentation (mixup, cutout)

10.4. Lessons Learned

1. **Transfer learning is powerful**: Pretrained ImageNet features work
   remarkably well for wildlife detection with minimal adaptation.

2. **DIoU > standard IoU**: The center distance penalty solves the vanishing
   gradient problem for non-overlapping boxes.

3. **Simplicity wins with small data**: Head-only training + pure DIoU
   outperformed more complex approaches.

4. **Engineering rigor matters**: Systematic ablation studies, proper logging,
   and failure analysis are as important as final accuracy.

==============================================================================
REFERENCES
==============================================================================

[1] Howard, A., et al. (2019). "Searching for MobileNetV3". ICCV 2019.

[2] Zheng, Z., et al. (2020). "Distance-IoU Loss: Faster and Better Learning 
    for Bounding Box Regression". AAAI 2020.

[3] Rezatofighi, H., et al. (2019). "Generalized Intersection over Union: 
    A Metric and A Loss for Bounding Box Regression". CVPR 2019.

[4] Yu, J., et al. (2016). "UnitBox: An Advanced Object Detection Network". 
    ACM Multimedia 2016.

[5] Girshick, R. (2015). "Fast R-CNN". ICCV 2015.

[6] Deng, J., et al. (2009). "ImageNet: A Large-Scale Hierarchical Image 
    Database". CVPR 2009.

==============================================================================
APPENDIX A: HYPERPARAMETER TUNING LOG
==============================================================================

Learning Rate Search:
- LR=0.001: Slow convergence (30 epochs ‚Üí 64% IoU)
- LR=0.002: ‚úì Best (22 epochs ‚Üí 67.2% IoU)  ‚Üê Selected
- LR=0.005: Unstable (oscillations, 65% IoU)

Batch Size:
- BS=8:  More stable, slower (45 min training)
- BS=16: ‚úì Best trade-off  ‚Üê Selected
- BS=32: Memory limits exceeded

Regularization:
- Dropout=0.2: Slight overfitting (gap=0.08)
- Dropout=0.3: ‚úì Optimal (gap=0.03)  ‚Üê Selected
- Dropout=0.5: Underfitting (max IoU=62%)

==============================================================================
END OF REPORT
==============================================================================
